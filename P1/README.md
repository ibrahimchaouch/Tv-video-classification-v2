# facepipe â€” Stageâ€‘1 Face Frequency & Embedding Pipeline

*A modular, reproducible implementation of the first stage (Face Appearance Frequency Analysis)*

> **Scope**: This repository implements **Stageâ€‘1** only â€” extracting **perâ€‘identity appearance frequency** and a **512â€‘D face embedding** bank from a TV video. Stages 2â€“3 (name assignment and web biography retrieval + final genre classification) are out of scope here and discussed in P2 & P3â€ and the complete pipeline motivation are in the paper; this repo focuses on the engineering details that make Stageâ€‘1 precise and fast.

---

## âœ¨ Highlights

* **Reader â†’ Detector(SCRFD) â†’ Aligner â†’ Embedder(ArcFace)** with **multiprocessing** and backâ€‘pressure queues.
* **Laplacianâ€‘based blur gate** applied **before detection** (global or grid mode) to skip lowâ€‘quality frames.
* **Identity clustering with online centroid update** and **cosine similarity** in normalized 512â€‘D space.
* **Redisâ€‘backed identity store** with **atomic matchâ€‘orâ€‘add** via a dedicated single writer process.
* Deterministic counters & run summary (elapsed time, detected/processed/saved faces, perâ€‘identity counts).

---

## ğŸ§  Method Summary

### 1) Preâ€‘filter (Frame Quality)

We compute the variance of the Laplacian on the gray frame (optionally in an $N\times N$ grid).

* Global metric: $v = \mathrm{Var}(\nabla^2 I)$. If $v < T$ the frame is skipped.
* Grid metric: a frame is kept if at least a fraction $\rho$ of tiles satisfy $\mathrm{Var}(\nabla^2 I_{tile}) \ge T$.

This gate eliminates blurry frames before detection and reduces false positives and wasted compute.

### 2) Detection (SCRFD, ONNX)

We use InsightFaceâ€™s **SCRFD** detector (`det_500m.onnx`) prepared on CPU. Input size is configurable (e.g., `640x640`). We forward the original BGR frame; for each detection whose width â‰¥ `--min-face`, we keep its 5â€‘point landmarks (when available).

### 3) Alignment

If 5â€‘point landmarks are available, we apply InsightFaceâ€™s `norm_crop` to produce a **112Ã—112** aligned face; otherwise we fall back to a tight box crop resized to 112Ã—112.

### 4) Embedding (ArcFace, ONNX)

We compute a 512â€‘D feature with **ArcFace** (`w600k_mbf.onnx`) and **L2â€‘normalize** it:

$$
\tilde{\mathbf{f}} = \frac{\mathbf{f}}{\lVert\mathbf{f}\rVert_2 + \varepsilon}\,,\qquad \tilde{\mathbf{f}}\in\mathbb{R}^{512},\; \lVert\tilde{\mathbf{f}}\rVert_2=1.
$$

### 5) Identity Matching with Online Centroid

Each Redis record stores at least `{embedding, centroid, refine_count, count}`. Given a new normalized feature $\tilde{\mathbf{f}}$, we search for the best key by cosine similarity $s = \tilde{\mathbf{c}}^\top\tilde{\mathbf{f}}$. If $s \ge \tau$ we **merge** into that identity; else we **create** a new one.

Centroid update (bounded by `--refine-limit`) uses a running mean **then reâ€‘normalizes**:


$$
\mathbf{c}_{t+1}
= \mathrm{norm}\!\left(\frac{r_t\,\mathbf{c}_t + \tilde{\mathbf{f}}}{r_t + 1}\right),
\qquad
r_{t+1} = \min\!\bigl(r_t + 1,\, \text{\texttt{refine\_limit}}\bigr).
$$

Frequency is tracked via `count â† count + 1` at each merge. This field is central to downstream stages.

### 6) Atomicity & Concurrency

All **matchâ€‘orâ€‘add** operations are **serialized** through a **dedicated Redis writer process** fed by a queue. This design ensures atomicity without perâ€‘key locks in readers. The writer performs the bestâ€‘match search and the update/create in one critical section per request.

---

## ğŸ—‚ï¸ Repository Layout

```
facepipe/
â”œâ”€ README.md
â”œâ”€ requirements.txt
â”œâ”€ config.py                  # defaults (paths, thresholds, ports)
â”œâ”€ main.py                    # orchestrates processes & queues
â”œâ”€ video/
â”‚   â””â”€ v1.mp4                 # sample (not in git)
â”œâ”€ models/                    # optional local notes
â”œâ”€ scripts/
â”‚   â””â”€ start_redis.sh         # start redis-server; flushall
â”œâ”€ pipeline/
â”‚   â”œâ”€ __init__.py
â”‚   â”œâ”€ reader.py              # frame reader process
â”‚   â”œâ”€ detector.py            # detector process (SCRFD + blur gate + align)
â”‚   â”œâ”€ embedding.py           # embedder workers
â”‚   â””â”€ redis_writer.py        # single writer (atomic match_or_add)
â”œâ”€ vision/
â”‚   â”œâ”€ __init__.py
â”‚   â”œâ”€ blur.py                # Laplacian metrics (global & grid)
â”‚   â”œâ”€ scrfd_wrap.py          # helper around insightface SCRFD
â”‚   â””â”€ arcface_wrap.py        # helper around ArcFace ONNX
â””â”€ utils/
    â”œâ”€ __init__.py
    â””â”€ redis_utils.py         # connect/cleanup helpers
```

---

## ğŸ”§ Installation

1. **Python env**

```bash
python -m venv .venv
source .venv/bin/activate
pip install -U pip
pip install -r requirements.txt
```

2. **Models**
   Install `insightface==0.7.3`. At first run, it will download the models automatically into `~/.insightface/models/models/buffalo_s/`:

* `det_500m.onnx` (SCRFD detector)
* `w600k_mbf.onnx` (ArcFace embedder)

3. **Redis**
   Make sure `redis-server` is available in `$PATH`.

---

## â–¶ï¸ Running

Start Redis (the script will also `FLUSHALL`):

```bash
chmod +x scripts/start_redis.sh
./scripts/start_redis.sh 6381
```

Run the pipeline:

```bash
python main.py \
  --video ./video/v1.mp4 \
  --det-model ~/.insightface/models/models/buffalo_s/det_500m.onnx \
  --rec-model ~/.insightface/models/models/buffalo_s/w600k_mbf.onnx \
  --det-input 640x640 --det-thresh 0.7 --min-face 60 \
  --frame-step 15 --num-det 2 --num-emb 6 \
  --redis-host localhost --redis-port 6381 --sim-thresh 0.27 --refine-limit 30 \
  --blur-enable --blur-var-thresh 120 --blur-grid 0 \
  --save-images --save-dir ./captures_faces \
  --debug
```

> If `--debug` is omitted, only the progress bars and the final summary are shown.

---

## ğŸ§° Commandâ€‘line Arguments (selected)

* **IO**: `--video`, `--save-images`, `--save-dir`
* **Detector**: `--det-model`, `--det-input=LxH`, `--det-thresh`, `--nms-thresh`, `--min-face`
* **Scheduling**: `--frame-step`
* **Parallelism**: `--num-det`, `--num-emb`, `--q-frames`, `--q-faces`
* **Redis**: `--redis-host`, `--redis-port`, `--redis-prefix`, `--sim-thresh`, `--refine-limit`
* **Blur gate**: `--blur-enable`, `--blur-var-thresh`, `--blur-grid`, `--blur-grid-min-keep`, `--blur-resize`
* **Logging**: `--debug` (verbose) / default (quiet)

---

## ğŸ“Š Output & Summary

At the end of a run the program prints:

* **Elapsed time** (from the first enqueued frame),
* **Faces detected â†’ queued**, **faces embedded**, **faces saved**,
* **Perâ€‘identity frequency** and a **global tally** read back from Redis (ground truth for Stageâ€‘1).

Saved crops (if `--save-images`) are organized as:

```
./captures_faces/
  â”œâ”€ <identity_id_A>/ frame_01234_XXXX.jpg â€¦
  â”œâ”€ <identity_id_B>/ â€¦
  â””â”€ â€¦
```

---

## ğŸ§ª Accuracyâ€‘preserving performance tips

* Keep `--det-input` at **640Ã—640** unless profiling shows a safe reduction.
* Increase `--frame-step` to skip frames uniformly when working with long videos.
* Use **multiple detector processes** (I/O bound) and **more embedder workers** (CPU bound).
* Use the **blur gate** to avoid spending compute on uninformative frames.
* Tune `--sim-thresh` around **0.25â€“0.30** with your data; too low merges distinct identities, too high splits.

---

## ğŸ”’ Notes on Atomicity

* All **matchâ€‘orâ€‘add** operations go through a **single writer**. This ensures that the bestâ€‘match search and the subsequent update/create happen as one serialized transaction per face.
* The writer is also responsible for updating the **frequency counter** (`count`) and the **centroid** with reâ€‘normalization.

---

## âœ… Reproducibility Notes

* We pin **`insightface==0.7.3`** to preserve the detector/ONNX call signatures.
* Models are CPU by default (`ctx_id=-1`). If a GPU provider is configured in ONNX Runtime, you can adapt the wrappers.

---

## ğŸ“¦ Requirements

See `requirements.txt`. Minimal set:

```
insightface==0.7.3
onnxruntime>=1.16
opencv-python>=4.8
numpy>=1.24
scipy>=1.10
scikit-learn
redis>=5.0
tqdm
```

---

## ğŸ“œ License

MIT. See `LICENSE`.

---

## âœï¸ Citation

If you find this useful, please cite the paper (details in the repository or the project page).
